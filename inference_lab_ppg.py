#!/usr/bin/env python
"""
Lab PPG Inference Script
=========================
Run inference on lab PPG data without requiring glucose labels.

This script:
1. Loads ppg_windows.csv (generated by generate_lab_training_data.py)
2. Normalizes the PPG data
3. Runs the trained model to predict glucose values
4. Saves predictions to a CSV file

Usage:
    python inference_lab_ppg.py --data_dir ./inference_data --model_path ./model/best_model.pth
"""

import os
import sys
import torch
import numpy as np
import pandas as pd
from collections import Counter

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from training.resnet34_glucose_predictor import ResNet34_1D


def load_ppg_windows(data_dir):
    """Load PPG windows from CSV file"""

    print("=" * 80)
    print("Loading PPG Windows")
    print("=" * 80)

    # Load PPG windows
    ppg_file = os.path.join(data_dir, 'ppg_windows.csv')
    print(f"Loading PPG windows from: {ppg_file}")

    if not os.path.exists(ppg_file):
        raise FileNotFoundError(f"PPG windows file not found: {ppg_file}")

    ppg_df = pd.read_csv(ppg_file)

    # Group by window_index and check lengths
    windows = []
    window_lengths = []
    for window_idx in sorted(ppg_df['window_index'].unique()):
        window_df = ppg_df[ppg_df['window_index'] == window_idx].sort_values('sample_index')
        window = window_df['amplitude'].values
        windows.append(window)
        window_lengths.append(len(window))

    # Find the most common window length
    length_counts = Counter(window_lengths)
    target_length = length_counts.most_common(1)[0][0]

    print(f"Window length statistics:")
    print(f"  Min length: {min(window_lengths)}")
    print(f"  Max length: {max(window_lengths)}")
    print(f"  Target length: {target_length}")
    print(f"  Total windows: {len(windows)}")

    # Pad or truncate windows to target length
    normalized_windows = []
    for window in windows:
        if len(window) == target_length:
            normalized_windows.append(window)
        elif len(window) > target_length:
            # Truncate
            normalized_windows.append(window[:target_length])
        else:
            # Pad with zeros
            padded = np.zeros(target_length)
            padded[:len(window)] = window
            normalized_windows.append(padded)

    ppg_data = np.array(normalized_windows)
    print(f"[OK] Loaded {len(ppg_data)} PPG windows")
    print(f"  Shape: {ppg_data.shape}")

    return ppg_data


def normalize_ppg(ppg_data):
    """Normalize PPG data (per-window normalization)"""
    ppg_mean = np.mean(ppg_data, axis=1, keepdims=True)
    ppg_std = np.std(ppg_data, axis=1, keepdims=True)
    ppg_std[ppg_std == 0] = 1.0  # Avoid division by zero
    normalized_ppg = (ppg_data - ppg_mean) / ppg_std
    return normalized_ppg


def run_inference(model_path, data_dir, output_dir=None, glucose_mean=None, glucose_std=None):
    """
    Run inference on PPG data to predict glucose values.

    Args:
        model_path: Path to trained model checkpoint
        data_dir: Directory containing ppg_windows.csv
        output_dir: Output directory for predictions (default: data_dir/predictions)
        glucose_mean: Glucose mean used during training (optional, overrides checkpoint)
        glucose_std: Glucose std used during training (optional, overrides checkpoint)
    """
    print("\n" + "=" * 80)
    print("LAB PPG GLUCOSE INFERENCE")
    print("=" * 80)
    print(f"Model: {model_path}")
    print(f"Data Directory: {data_dir}")

    # Load PPG data
    ppg_data = load_ppg_windows(data_dir)

    # Normalize data (same as training)
    print("\n" + "=" * 80)
    print("Normalizing PPG Data")
    print("=" * 80)

    ppg_normalized = normalize_ppg(ppg_data)
    print(f"[OK] PPG data normalized")
    print(f"  Normalized range: {ppg_normalized.min():.4f} - {ppg_normalized.max():.4f}")

    # Load model
    print("\n" + "=" * 80)
    print("Loading Model")
    print("=" * 80)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")

    input_length = ppg_data.shape[1]
    model = ResNet34_1D(input_length=input_length, num_classes=1)

    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    print(f"[OK] Model loaded successfully")
    print(f"  Epoch: {checkpoint['epoch']}")

    # Get normalization parameters (priority: args > checkpoint > defaults)
    if glucose_mean is None:
        glucose_mean = checkpoint.get('glucose_mean', 0.0)
    if glucose_std is None:
        glucose_std = checkpoint.get('glucose_std', 1.0)

    print(f"\nGlucose denormalization parameters:")
    print(f"  Mean: {glucose_mean:.2f} mg/dL")
    print(f"  Std: {glucose_std:.2f} mg/dL")

    if glucose_mean == 0.0 and glucose_std == 1.0:
        print("  [WARNING] Using default normalization parameters (0.0, 1.0)")
        print("  [WARNING] Predictions may not be in correct mg/dL scale")
        print("  [HINT] Use --glucose_mean and --glucose_std arguments to specify training normalization")

    # Run predictions
    print("\n" + "=" * 80)
    print("Running Predictions")
    print("=" * 80)

    all_predictions_normalized = []

    batch_size = 32
    num_samples = len(ppg_normalized)

    with torch.no_grad():
        for i in range(0, num_samples, batch_size):
            # Get batch (handles last batch correctly)
            end_idx = min(i + batch_size, num_samples)
            batch_ppg = ppg_normalized[i:end_idx]

            # Convert to tensor and add channel dimension
            batch_tensor = torch.tensor(batch_ppg, dtype=torch.float32).unsqueeze(1).to(device)

            # Predict (outputs normalized values)
            predictions = model(batch_tensor)

            # Append predictions
            batch_predictions = predictions.cpu().numpy().flatten()
            all_predictions_normalized.extend(batch_predictions)

    predictions_normalized = np.array(all_predictions_normalized)

    print(f"[OK] Generated {len(predictions_normalized)} predictions")
    print(f"  Normalized predictions range: {predictions_normalized.min():.4f} - {predictions_normalized.max():.4f}")

    # Denormalize predictions
    print("\n" + "=" * 80)
    print("Denormalizing Predictions")
    print("=" * 80)

    predictions_mgdl = predictions_normalized * glucose_std + glucose_mean

    print(f"Predicted glucose values:")
    print(f"  Range: {predictions_mgdl.min():.2f} - {predictions_mgdl.max():.2f} mg/dL")
    print(f"  Mean: {predictions_mgdl.mean():.2f} mg/dL")
    print(f"  Std: {predictions_mgdl.std():.2f} mg/dL")
    print(f"  Median: {np.median(predictions_mgdl):.2f} mg/dL")

    # Clinical interpretation
    print("\nClinical Interpretation (based on mean):")
    mean_glucose = predictions_mgdl.mean()
    if mean_glucose < 70:
        print(f"  [LOW] Hypoglycemia: {mean_glucose:.2f} mg/dL < 70 mg/dL")
    elif mean_glucose <= 100:
        print(f"  [NORMAL] Normal fasting: 70 ≤ {mean_glucose:.2f} ≤ 100 mg/dL")
    elif mean_glucose <= 125:
        print(f"  [ELEVATED] Prediabetes: 100 < {mean_glucose:.2f} ≤ 125 mg/dL")
    else:
        print(f"  [HIGH] Diabetes range: {mean_glucose:.2f} > 125 mg/dL")

    # Save results
    if output_dir is None:
        output_dir = os.path.join(data_dir, 'predictions')

    os.makedirs(output_dir, exist_ok=True)

    results_df = pd.DataFrame({
        'window_index': range(len(predictions_mgdl)),
        'predicted_glucose_mg_dl': predictions_mgdl
    })

    output_file = os.path.join(output_dir, 'glucose_predictions.csv')
    results_df.to_csv(output_file, index=False)
    print(f"\n[OK] Predictions saved to: {output_file}")

    # Show sample predictions
    print("\nSample Predictions (first 20 windows):")
    print(f"{'Window':<10} {'Predicted Glucose (mg/dL)':<25}")
    print("-" * 35)
    for i in range(min(20, len(predictions_mgdl))):
        print(f"{i:<10} {predictions_mgdl[i]:<25.2f}")

    if len(predictions_mgdl) > 20:
        print(f"... and {len(predictions_mgdl) - 20} more windows")

    print("\n" + "=" * 80)
    print("INFERENCE COMPLETED")
    print("=" * 80)
    print(f"\nSummary:")
    print(f"  Total windows processed: {len(predictions_mgdl)}")
    print(f"  Average predicted glucose: {predictions_mgdl.mean():.2f} mg/dL")
    print(f"  Predictions saved to: {output_file}")

    return predictions_mgdl


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(
        description='Run glucose inference on lab PPG data',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python inference_lab_ppg.py --data_dir ./inference_data --model_path ./model/best_model.pth

  # Specify output directory
  python inference_lab_ppg.py --data_dir ./inference_data --model_path ./model/best_model.pth --output ./results
        """
    )

    parser.add_argument('--model_path', type=str, required=True,
                       help='Path to trained model checkpoint (.pth file)')
    parser.add_argument('--data_dir', type=str, required=True,
                       help='Directory containing ppg_windows.csv')
    parser.add_argument('--output', type=str, default=None,
                       help='Output directory for predictions (default: data_dir/predictions)')
    parser.add_argument('--glucose_mean', type=float, default=None,
                       help='Glucose mean used during training (overrides checkpoint value)')
    parser.add_argument('--glucose_std', type=float, default=None,
                       help='Glucose std used during training (overrides checkpoint value)')

    args = parser.parse_args()

    # Validate inputs
    if not os.path.exists(args.model_path):
        print(f"[ERROR] Model file not found: {args.model_path}")
        sys.exit(1)

    if not os.path.exists(args.data_dir):
        print(f"[ERROR] Data directory not found: {args.data_dir}")
        sys.exit(1)

    ppg_file = os.path.join(args.data_dir, 'ppg_windows.csv')
    if not os.path.exists(ppg_file):
        print(f"[ERROR] ppg_windows.csv not found in: {args.data_dir}")
        print(f"Expected file: {ppg_file}")
        sys.exit(1)

    # Run inference
    try:
        run_inference(args.model_path, args.data_dir, args.output,
                     args.glucose_mean, args.glucose_std)
    except Exception as e:
        print(f"\n[ERROR] Inference failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
